configfile: "config/config.yaml"

rule all:
    input:
        expand("results/{sample}.{word_length}mers.expr_enriched_tfs.txt", word_length=config["word_length"], sample=config["sample"])

rule generate_motif_database:
    input:
        expand("data/tf_database/{tf_database_file}.{word_length}_{min_word_length}mers.fimo.txt", tf_database_file=config['tf_database_file'], word_length=config["word_length"], min_word_length=config["min_word_length"])

#python lsgkm/scripts/nrkmers.py 11 testrun/11mers.fa
rule get_kmers:
    output: 
        protected("data/{word_length}mers.fa")
    script:
        "scripts/nrkmers.py"

rule split_motifs:
    input:
        "data/tf_database/{tf_database_file}.txt",
    output:
        temp(expand("temp/{{tf_database_file}}.{i}.txt", i=range(workflow.cores)))
    params:
        target_fn="temp/{tf_database_file}"
    script:
        "scripts/split_file.py"

rule run_fimo:
    input:
        kmers = "data/{word_length}mers.fa",
        db_file = "temp/{tf_database_file}.{i}.txt"
    output:
        temp("temp/{tf_database_file}.{word_length}mers.{i}.fimo.txt")
    shell:
        "fimo --skip-matched-sequence {input.db_file} {input.kmers} > {output}"

rule combine_fimo_output:
    input: 
        expand("temp/{{tf_database_file}}.{{word_length}}mers.{i}.fimo.txt", i=range(workflow.cores))
    output:
        protected("data/tf_database/{tf_database_file}.{word_length}_{min_word_length}mers.fimo.txt")
    shell:
        """
        cat {input} | grep -v 'motif' > tmp
        cut -f 2 tmp | \
        cut -d '_' -f 1 | \
        paste - tmp | \
        cut -f 1,2,4 | \
        sort -u > {output}
        rm tmp
        """
        
#lsgkm/bin/gkmtrain lsgkm/tests/wgEncodeSydhTfbsGm12878Nfe2hStdAlnRep0.tr.fa lsgkm/tests/wgEncodeSydhTfbsGm12878Nfe2hStdAlnRep0.neg.tr.fa testrun/test_gkmtrain
rule gkm_train:
    input:
        pos = "data/samples/{sample}.tr.fa",
        neg = "data/samples/{sample}.neg.tr.fa"
    output:
        "model/{sample}.{word_length}mers.model.txt"
    shell:
        "../lsgkm/bin/gkmtrain -l {wildcards.word_length} -d 2 {input.pos} {input.neg} model/{wildcards.sample}.{wildcards.word_length}mers"


#lsgkm/bin/gkmpredict testrun/11mers.fa testrun/test_gkmtrain.model.txt test_svmw.txt
rule gkm_predict:
    input:
        kmers = "data/{word_length}mers.fa",
        model = "model/{sample}.{word_length}mers.model.txt"
    output:
        "weights/{sample}.{word_length}mers.svmw.txt"
    shell:
        "../lsgkm/bin/gkmpredict {input.kmers} {input.model} {output}"

#./count_kmers_per_motif.py jaspar/fimo.JASPAR2022_CORE_vertebrates_non-redundant_pfms_meme.11mers.uniq.txt testrun/test_svmw.txt test 0.1 0.01 0.01 TF_fam.txt
rule enriched_tfs:
    input:
        "weights/{sample}.{word_length}mers.svmw.txt",
        expand("data/tf_database/{tf_database_file}.{{word_length}}_{min_word_length}mers.fimo.txt", tf_database_file=config['tf_database_file'], min_word_length=config["min_word_length"])
    output:
        "results/{sample}.{word_length}mers.expr_enriched_tfs.txt"
    params:
        tf_metadata_file = config['tf_metadata_file'],
        percent_kmer_5p = config['enrichment_cutoffs']['percent_kmer_5p'],
        padj_cutoff_poisson = config['enrichment_cutoffs']['padj_cutoff_poisson'],
        padj_cutoff_wilcox = config['enrichment_cutoffs']['padj_cutoff_wilcox']
    shell:
        "python -E workflow/scripts/count_kmers_per_motif.py {input} results/{wildcards.sample}.{wildcards.word_length}mers {params.percent_kmer_5p} {params.padj_cutoff_poisson} {params.padj_cutoff_wilcox} data/tf_database/{params.tf_metadata_file}.txt"